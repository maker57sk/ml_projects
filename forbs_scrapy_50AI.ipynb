{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forbs_scrapy_50AI.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKzEHUbDCXmCJFxwpa3vml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maker57sk/ml_projects/blob/master/forbs_scrapy_50AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!pip3 install scrapy\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "lsFn2qmRJ3bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdCdeEC4JtQT",
        "outputId": "4e1c5fd5-325d-45a9-9c09-ad838c70cb4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-10 04:21:35 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
            "2022-05-10 04:21:35 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.4.0, Python 3.7.13 (default, Apr 24 2022, 01:04:09) - [GCC 7.5.0], pyOpenSSL 22.0.0 (OpenSSL 3.0.3 3 May 2022), cryptography 37.0.2, Platform Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2022-05-10 04:21:35 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2022-05-10 04:21:35 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2022-05-10 04:21:35 [scrapy.extensions.telnet] INFO: Telnet Password: 756363f7f251bc68\n",
            "2022-05-10 04:21:35 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2022-05-10 04:21:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2022-05-10 04:21:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2022-05-10 04:21:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2022-05-10 04:21:35 [scrapy.core.engine] INFO: Spider opened\n",
            "2022-05-10 04:21:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2022-05-10 04:21:36 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2022-05-10 04:21:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.forbes.com/lists/ai50/> (referer: None)\n",
            "2022-05-10 04:21:36 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2022-05-10 04:21:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 225,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 90667,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.337891,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2022, 5, 10, 4, 21, 36, 447930),\n",
            " 'httpcompression/response_bytes': 482718,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 2,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 139952128,\n",
            " 'memusage/startup': 139952128,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2022, 5, 10, 4, 21, 36, 110039)}\n",
            "2022-05-10 04:21:36 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6sense\n",
            "Abacus.AI\n",
            "Abnormal Security\n",
            "Amira Learning\n",
            "AMP Robotics\n",
            "Anyscale\n",
            "Arize AI\n",
            "ASAPP\n",
            "Aurora Solar\n",
            "Brain Technologies\n",
            "Brightseed\n",
            "Canvas\n",
            "ClosedLoop\n",
            "Cohere\n",
            "ConverseNow\n",
            "Cresta\n",
            "Databricks\n",
            "Dataiku\n",
            "Deepcell\n",
            "Domino Data Lab\n",
            "Eigen Technologies\n",
            "Entos\n",
            "Facet\n",
            "FarmWise\n",
            "Forethought\n",
            "Generate Biomedicines\n",
            "Genesis Therapeutics\n",
            "Glean\n",
            "Hugging Face\n",
            "Intenseye\n",
            "Kintsugi Mindful Wellness\n",
            "Komodo Health\n",
            "Labelbox\n",
            "Mashgin\n",
            "Matroid\n",
            "MetaMap\n",
            "Moveworks\n",
            "Mutiny\n",
            "Nauto\n",
            "Netradyne\n",
            "Nuro\n",
            "OctoML\n",
            "Overjet\n",
            "Scale AI\n",
            "Sisu Data\n",
            "Sprig\n",
            "Uniphore\n",
            "Verge Genomics\n",
            "Waabi\n",
            "Whisper\n"
          ]
        }
      ],
      "source": [
        "# Import the scrapy library\n",
        "import scrapy\n",
        "# Import a special class called CrawlerProcess from the folder scrapy.crawler\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "# Import openpyxl for reading and writing Excel documents\n",
        "from openpyxl import Workbook\n",
        "\n",
        "# Instantiate a new workbook\n",
        "wb = Workbook()\n",
        "\n",
        "# Choose the active worksheet in the workbook\n",
        "ws = wb.active\n",
        "\n",
        "class WebSpider(scrapy.Spider):\n",
        "\n",
        "    name = \"WebSpider\"\n",
        "\n",
        "    # For more examples, visit http://toscrape.com/\n",
        "    start_urls = [\n",
        "        \"https://www.forbes.com/lists/ai50/\",\n",
        "    ]\n",
        "\n",
        "    # Scrapy will pass the argument for \"webPage\"\n",
        "    # after it runs the spider\n",
        "    def parse(self, webPage):\n",
        "\n",
        "        # Using CSS to find the card\n",
        "        # theCards will a List containing HTML elements\n",
        "        theCards = webPage.css( 'html body.list-lander div.main-content main.et-promoblock div.table-block section#row-2 ul.promo-block-list  li.csf-column div.csf-block div.table-wrapper div.grid-wrapper div.table div.table-row-group a.table-row')\n",
        "#html.no-js body#default.default div.container-fluid.page div.page_inner div.row div.col-sm-8.col-md-9 section div ol.row li.col-xs-6.col-sm-4.col-md-3.col-lg-3\n",
        "        # Using Xpath to find the card\n",
        "        # theCards = webPage.selector.xpath('/html/body/div/div/div/div/section/div[2]/ol/li[1]').get()\n",
        "\n",
        "        # Loop through all of the cards\n",
        "        for card in theCards:\n",
        "\n",
        "            # Create an empty list to populate later\n",
        "            excelRow = []\n",
        "\n",
        "            # (1) Get the 'name' of the property\n",
        "            # .css() method will retrieve the HTML element\n",
        "            # .get() method will convert the HTML element to Python string\n",
        "            name = card.css('div.organizationName ::text').get()\n",
        "            industry = card.css('div.industries ::text').get()\n",
        "            funding = card.css('div.funding ::text').get()\n",
        "            hq = card.css('div.headquarters ::text').get()\n",
        "            ceo = card.css('div.ceoName ::text').get()\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            # Test that the CSS path is correct\n",
        "            print(name)\n",
        "            # print(industry)\n",
        "\n",
        "            # Add the name to the row \n",
        "            excelRow.append(name)\n",
        "            excelRow.append(industry)\n",
        "            excelRow.append(funding)\n",
        "            excelRow.append(hq)\n",
        "            excelRow.append(ceo)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Add the excelRow to the worksheet\n",
        "            ws.append(excelRow)\n",
        "\n",
        "        # Example for paginating by clicking on the next page\n",
        "        # for next_page in webPage.css('html.no-js body#default.default div.container-fluid.page div.page_inner div.row div.col-sm-8.col-md-9 section div div ul.pager li.next a'):\n",
        "        #      yield webPage.follow(next_page, self.parse)\n",
        "\n",
        "        # Save the whole workbook       \n",
        "        wb.save('forbs_50ai.xlsx')\n",
        "\n",
        "\n",
        "# Instantiate and run our WebSpider\n",
        "process = CrawlerProcess()\n",
        "process.crawl(WebSpider)\n",
        "process.start()"
      ]
    }
  ]
}